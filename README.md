# Prompt-engineering

## Executive Overview
Prompt engineering is the specific style of communication to provide sufficient context, instructions, examples to LLM models in order to minimise uncertainty and thus guesswork that the model has to do. 

With key prompts, LLM use can be a tool for amplifiying what one's skills already are, instead of using it as a crutch. 

It can only multiply what is already there. *Good and bad habits* are amplified

## Tips & Tricks
- **Master the fundamentals first**. Do courses online on the tool instead of jumping in blindly with 0 knowledge thus going in needing a crutch
  - Online courses from microsoft, google, AWS on the most common tools.
-  Be as specific and technical as possible. [Prompt engineering 101](https://services.google.com/fh/files/misc/gemini-for-google-workspace-prompting-guide-101.pdf) 
  - In a prompt have: Persona, Task, Context, Example, Output format
- Break it down the problem as much as possible. The smaller the task, the better the result.
  - If you cannot break down the problem, you do not understand the problem well enough, i.e. problem-solving
    1. Plan out a solution
    2. Breaking the problem down into functions/componets
    3. Document to easily retrace steps
- Tell AI **what you DO NOT want**
  - Have a 'Do not' section, constraining the solution space
- Give AI a way to verify its code.
  - Test with sample datasets, test environments, edge case parsing etc.
- Tell AI to remember, drop in a agent.md for context.


## References
[YT vid that started this](https://www.youtube.com/watch?v=91B_v-wOaws)

  
